{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer #loving = love\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "#import dask.dataframe as dd\n",
    "#import dask.array as da\n",
    "#import dask.delayed as dl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading stopwords\n",
    "\n",
    "nltk.download('stopwords') # stopwords são preposições, 'this', 'that',..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame.from_csv('train.tsv', sep='\\t')\n",
    "test = pd.DataFrame.from_csv('test.tsv', sep='\\t')\n",
    "y_train = train['Sentiment']\n",
    "PhraseId = test.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 156060 entries, 1 to 156060\n",
      "Data columns (total 3 columns):\n",
      "SentenceId    156060 non-null int64\n",
      "Phrase        156060 non-null object\n",
      "Sentiment     156060 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhraseId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SentenceId                                             Phrase  \\\n",
       "PhraseId                                                                  \n",
       "1                  1  A series of escapades demonstrating the adage ...   \n",
       "2                  1  A series of escapades demonstrating the adage ...   \n",
       "3                  1                                           A series   \n",
       "4                  1                                                  A   \n",
       "5                  1                                             series   \n",
       "6                  1  of escapades demonstrating the adage that what...   \n",
       "7                  1                                                 of   \n",
       "8                  1  escapades demonstrating the adage that what is...   \n",
       "9                  1                                          escapades   \n",
       "10                 1  demonstrating the adage that what is good for ...   \n",
       "\n",
       "          Sentiment  \n",
       "PhraseId             \n",
       "1                 1  \n",
       "2                 2  \n",
       "3                 2  \n",
       "4                 2  \n",
       "5                 2  \n",
       "6                 2  \n",
       "7                 2  \n",
       "8                 2  \n",
       "9                 2  \n",
       "10                2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus for training set\n",
    "corpus_train = [] # Initializing an empty list\n",
    "for i in range(0, len(train)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', train.iloc[i, 1]) # Replacing non letters with empty spaces\n",
    "    review = review.lower() # Getting lowers\n",
    "    review = review.split() # Splitting on spaces - creating a vector\n",
    "    ps = PorterStemmer() \n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] # Iterating over vector excluding stopwords\n",
    "    review = ' '.join(review) # to string\n",
    "    corpus_train.append(review) # append on corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am gonna save this corpus into a pickle file, because this process is computacionally expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load from file\n",
    "f = open(\"corpus_train.pickle\",\"rb\")\n",
    "corpus_train = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060,)\n",
      "['seri escapad demonstr adag good goos also good gander occasion amus none amount much stori', 'seri escapad demonstr adag good goos', 'seri', '', 'seri', 'escapad demonstr adag good goos', '', 'escapad demonstr adag good goos', 'escapad', 'demonstr adag good goos', 'demonstr adag', 'demonstr', 'adag', '', 'adag', 'good goos', '', 'good goos', '', 'good goos']\n"
     ]
    }
   ],
   "source": [
    "print(np.array(corpus_train).shape)\n",
    "print(corpus_train[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the part where we have to vectorize our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_train = pd.DataFrame(corpus_train)\n",
    "#corpus_train = dd.from_pandas(corpus_train, npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose vectorize method\n",
    "\n",
    "# \"tfidf\", \"cv\" or \"hv\"\n",
    "vectorize_method = \"hv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (vectorize_method == \"tfidf\"):\n",
    "    tfidf = TfidfVectorizer(norm='l2', min_df=0, use_idf=True, smooth_idf=False,\n",
    "                        sublinear_tf=True, max_features=6918)\n",
    "    corpus_train = tfidf.fit_transform(corpus_train)\n",
    "elif (vectorize_method == \"cv\"):\n",
    "    cv = CountVectorizer() \n",
    "    corpus_train = cv.fit_transform(corpus_train)\n",
    "elif (vectorize_method == \"hv\"):\n",
    "    hv = HashingVectorizer(decode_error='ignore', n_features=2 ** 14) \n",
    "    corpus_train = hv.transform(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060, 16384)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tsvd = TruncatedSVD(n_components = 6918, random_state=42)\n",
    "#corpus_train = tsvd.transform(corpus_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus for test set\n",
    "\n",
    "corpus_test = [] # Initializing an empty list\n",
    "for i in range(0, len(test)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', test.iloc[i, 1]) # Replacing non letters with empty spaces\n",
    "    review = review.lower() # Getting lowers\n",
    "    review = review.split() # Splitting on spaces - creating a vector\n",
    "    ps = PorterStemmer() \n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] # Iterating over vector excluding stopwords\n",
    "    review = ' '.join(review) # to string\n",
    "    corpus_test.append(review) # append on corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load from file\n",
    "f = open(\"corpus_test.pickle\",\"rb\")\n",
    "corpus_test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66292,)\n",
      "['intermitt pleas mostli routin effort', 'intermitt pleas mostli routin effort', '', 'intermitt pleas mostli routin effort', 'intermitt pleas mostli routin', 'intermitt pleas', 'intermitt pleas', 'intermitt', 'pleas', '']\n"
     ]
    }
   ],
   "source": [
    "print(np.array(corpus_test).shape)\n",
    "print(corpus_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_test = pd.DataFrame(corpus_test)\n",
    "#corpus_test = dd.from_pandas(corpus_test, npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the bag of words model and vectorizing\n",
    "\n",
    "if (vectorize_method == \"tfidf\"):\n",
    "    tfidf = TfidfVectorizer(norm='l2', min_df=0, use_idf=True, smooth_idf=False,\n",
    "                            sublinear_tf=True)\n",
    "    corpus_test = tfidf.fit_transform(corpus_test)\n",
    "elif (vectorize_method == \"cv\"):\n",
    "    cv = CountVectorizer() \n",
    "    corpus_test = cv.fit_transform(corpus_test)\n",
    "elif (vectorize_method == \"hv\"):\n",
    "    hv = HashingVectorizer(decode_error='ignore', n_features=2 ** 14) \n",
    "    corpus_test = hv.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_test = da.from_array(corpus_test, chunks=(1000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66292, 16384)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing hyperparameters for the classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definning params for classifiers\n",
    "\n",
    "params_rf = {'n_estimators': 100,\n",
    "             'criterion': 'entropy',\n",
    "             'n_jobs': 2,\n",
    "             'random_state': 42,\n",
    "             'verbose': 2,\n",
    "             'max_features': 0.2,\n",
    "             'min_samples_leaf': 5\n",
    "}\n",
    "\n",
    "params_etc = {'n_estimators': 100,\n",
    "              'criterion': 'entropy',\n",
    "              'max_depth': None,\n",
    "              'min_samples_split': 2,\n",
    "              'min_samples_leaf': 1,\n",
    "              'n_jobs': 2,\n",
    "              'random_state': 42,\n",
    "              'verbose': 2\n",
    "}\n",
    "\n",
    "params_ada = {'n_estimators': 100,\n",
    "              'learning_rate': 1,\n",
    "              'random_state': 42,\n",
    "             }\n",
    "\n",
    "params_gtb = {'loss': 'deviance',\n",
    "              'learning_rate': 1,\n",
    "              'n_estimators': 100,\n",
    "              'random_state': 42,\n",
    "              'verbose': 2\n",
    "}\n",
    "\n",
    "params_lr = {'multi_class': 'multinomial',\n",
    "             'random_state': 42,\n",
    "              'verbose': 2,\n",
    "              'n_jobs': 2,\n",
    "             'solver': 'sag'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the classifiers\n",
    "clf_rf = RandomForestClassifier(**params_rf)\n",
    "clf_etc = ExtraTreesClassifier(**params_etc)\n",
    "clf_ada = AdaBoostClassifier(**params_ada)\n",
    "clf_gtb = GradientBoostingClassifier(**params_gtb)\n",
    "clf_mnb = MultinomialNB()\n",
    "clf_bnb = BernoulliNB()\n",
    "clf_lr = LogisticRegression(**params_lr)\n",
    "clf_xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n"
     ]
    }
   ],
   "source": [
    "clf_ens = VotingClassifier(estimators=[('rf', clf_rf), \n",
    "                                       ('etc', clf_etc), \n",
    "                                       ('ada', clf_ada), \n",
    "                                       ('gtb', clf_gtb),\n",
    "                                       ('mnb', clf_mnb),\n",
    "                                       ('bnb', clf_bnb),\n",
    "                                       ('lr', clf_lr),\n",
    "                                       ('xgb', clf_xgb)],\n",
    "                           voting='hard',\n",
    "                           n_jobs = 2)\n",
    "\n",
    "clf_ens.fit(corpus_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame({\"PhraseId\": PhraseId, \"Sentiment\": clf_ens.predict(corpus_test)}).to_csv(\"./results/results_ens.csv\", index=None)\n",
    "PhraseId = test.index.values\n",
    "pd.DataFrame({\"PhraseId\": PhraseId, \n",
    "              \"Sentiment\": clf_ens.predict(corpus_test)}).to_csv(\"./results/results_ens.csv\", \n",
    "                                                                index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_train, clf_ens.estimators_[3].predict(corpus_train))\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    pd.DataFrame({\"PhraseId\": PhraseId, \n",
    "                  \"Sentiment\": clf_ens.estimators_[i].predict(corpus_test)}).to_csv(\"./results/results_{}.csv\".format(i), \n",
    "                                                                                    index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
