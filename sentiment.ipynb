{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer #loving = love\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "#import dask.dataframe as dd\n",
    "#import dask.array as da\n",
    "#import dask.delayed as dl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading stopwords\n",
    "\n",
    "nltk.download('stopwords') # stopwords são preposições, 'this', 'that',..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame.from_csv('train.tsv', sep='\\t')\n",
    "test = pd.DataFrame.from_csv('test.tsv', sep='\\t')\n",
    "y_train = train['Sentiment']\n",
    "PhraseId = test.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus for training set\n",
    "corpus_train = [] # Initializing an empty list\n",
    "for i in range(0, len(train)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', train.iloc[i, 1]) # Replacing non letters with empty spaces\n",
    "    review = review.lower() # Getting lowers\n",
    "    review = review.split() # Splitting on spaces - creating a vector\n",
    "    ps = PorterStemmer() \n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] # Iterating over vector excluding stopwords\n",
    "    review = ' '.join(review) # to string\n",
    "    corpus_train.append(review) # append on corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am gonna save this corpus into a pickle file, because this process is computacionally expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load from file\n",
    "f = open(\"corpus_train.pickle\",\"rb\")\n",
    "corpus_train = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(corpus_train).shape)\n",
    "print(corpus_train[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the part where we have to vectorize our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_train = pd.DataFrame(corpus_train)\n",
    "#corpus_train = dd.from_pandas(corpus_train, npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose vectorize method\n",
    "\n",
    "# \"tfidf\", \"cv\" or \"hv\"\n",
    "vectorize_method = \"hv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (vectorize_method == \"tfidf\"):\n",
    "    tfidf = TfidfVectorizer(norm='l2', min_df=0, use_idf=True, smooth_idf=False,\n",
    "                        sublinear_tf=True, max_features = 1500)\n",
    "    corpus_train = tfidf.fit_transform(X[0]).toarray()\n",
    "elif (vectorize_method == \"cv\"):\n",
    "    cv = CountVectorizer(max_features = 5000) \n",
    "    corpus_train = cv.fit_transform(corpus_train).toarray()\n",
    "elif (vectorize_method == \"hv\"):\n",
    "    hv = HashingVectorizer(decode_error='ignore', n_features=2 ** 13) \n",
    "    corpus_train = hv.transform(corpus_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060, 8192)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = pd.DataFrame(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train.max(axis=1).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 800)\n",
    "corpus_train = pca.fit_transform(corpus_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus for test set\n",
    "\n",
    "corpus_test = [] # Initializing an empty list\n",
    "for i in range(0, len(test)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', test.iloc[i, 1]) # Replacing non letters with empty spaces\n",
    "    review = review.lower() # Getting lowers\n",
    "    review = review.split() # Splitting on spaces - creating a vector\n",
    "    ps = PorterStemmer() \n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] # Iterating over vector excluding stopwords\n",
    "    review = ' '.join(review) # to string\n",
    "    corpus_test.append(review) # append on corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load from file\n",
    "f = open(\"corpus_test.pickle\",\"rb\")\n",
    "corpus_test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(corpus_test).shape)\n",
    "print(corpus_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_test = pd.DataFrame(corpus_test)\n",
    "#corpus_test = dd.from_pandas(corpus_test, npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the bag of words model and vectorizing\n",
    "\n",
    "if (vectorize_method == \"tfidf\"):\n",
    "    tfidf = TfidfVectorizer(norm='l2', min_df=0, use_idf=True, smooth_idf=False,\n",
    "                            sublinear_tf=True, max_features = 1500)\n",
    "    corpus_test = tfidf.fit_transform(corpus_test).toarray()\n",
    "elif (vectorize_method == \"cv\"):\n",
    "    cv = CountVectorizer(max_features = 5000) \n",
    "    corpus_test = cv.fit_transform(corpus_test).toarray()\n",
    "elif (vectorize_method == \"hv\"):\n",
    "    hv = HashingVectorizer(decode_error='ignore', n_features=2 ** 13) \n",
    "    corpus_test = hv.transform(corpus_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_test = da.from_array(corpus_test, chunks=(1000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing hyperparameters for the classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definning params for classifiers\n",
    "\n",
    "params_rf = {'n_estimators': 50,\n",
    "             'criterion': 'entropy',\n",
    "             'n_jobs': 2,\n",
    "             'random_state': 42,\n",
    "             'verbose': 2,\n",
    "             'max_features': 0.2,\n",
    "             'min_samples_leaf': 5\n",
    "}\n",
    "\n",
    "params_etc = {'n_estimators': 50,\n",
    "              'criterion': 'entropy',\n",
    "              'max_depth': None,\n",
    "              'min_samples_split': 2,\n",
    "              'min_samples_leaf': 1,\n",
    "              'n_jobs': 2,\n",
    "              'random_state': 42,\n",
    "              'verbose': 2\n",
    "}\n",
    "\n",
    "params_ada = {'n_estimators': 50,\n",
    "              'learning_rate': 1,\n",
    "              'random_state': 42,\n",
    "             }\n",
    "\n",
    "params_gtb = {'loss': 'deviance',\n",
    "              'learning_rate': 1,\n",
    "              'n_estimators': 50,\n",
    "              'random_state': 42,\n",
    "              'verbose': 2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the classifiers\n",
    "clf_rf = RandomForestClassifier(**params_rf)\n",
    "clf_etc = ExtraTreesClassifier(**params_etc)\n",
    "clf_ada = AdaBoostClassifier(**params_ada)\n",
    "clf_gtb = GradientBoostingClassifier(**params_gtb)\n",
    "#clf_mnb = MultinomialNB()\n",
    "#clf_bnv = BernoulliNB()\n",
    "#clf_svm = SVC(kernel=\"rbf\", random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf_svm.fit(corpus_train, train[\"Sentiment\"])\n",
    "#pd.DataFrame({\"PhraseId\": PhraseId, \n",
    "#              \"Sentiment\": clf_svm.predict(corpus_test)}).to_csv(\"./results/results_svm.csv\", \n",
    "#                                                                 index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf.fit(corpus_train, train[\"Sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open(\"clf_rf.pickle\", \"wb\")\n",
    "#pickle.dump(clf_rf, f)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ens = VotingClassifier(estimators=[('rf', clf_rf), \n",
    "                                       ('etc', clf_etc), \n",
    "                                       ('ada', clf_ada), \n",
    "                                       ('gtb', clf_gtb)],\n",
    "                           voting='hard',\n",
    "                           n_jobs = 2)\n",
    "\n",
    "clf_ens.fit(corpus_train.compute(), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame({\"PhraseId\": PhraseId, \"Sentiment\": clf_ens.predict(corpus_test)}).to_csv(\"./results/results_ens.csv\", index=None)\n",
    "PhraseId = test.index.values\n",
    "pd.DataFrame({\"PhraseId\": PhraseId, \n",
    "              \"Sentiment\": clf_ens.predict(corpus_test)}).to_csv(\"./results/results_ens.csv\", \n",
    "                                                                index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_train, clf_ens.estimators_[3].predict(corpus_train))\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    pd.DataFrame({\"PhraseId\": PhraseId, \n",
    "                  \"Sentiment\": clf_ens.estimators_[i].predict(corpus_test)}).to_csv(\"./results/results_{}.csv\".format(i), \n",
    "                                                                                    index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
